# News Center Project

We use Python 3 virtualenv

to activate virtual environment run `source venv/bin/activate`

`cd spider/chinatimes`
`scrapy crawl news -o chinatimes.csv`

Deploy project

`scrapyd-deploy`

Scrapy structure
![alt text](https://docs.scrapy.org/en/latest/_images/scrapy_architecture_02.png "scrapy structure")


# Current design:
## Spider
Crawl through some start urls, for example:

| Site | Start Url            |
|------|----------------------|
| 中時 | www.chinatimes.com   |
| 聯合 | udn.com              |
| 三立 | www.setn.com |

Follow for depth N, and only yield item if is artile.

## Item Pipeline
* We are planning to store crawled HTML in Google Cloud Storage. For example if we crawl a page:
  `https://www.chinatimes.com/realtimenews/<news_id>?chdtv`
  We will store it under `gs://twnews-rawdata/www.chinatimes.com/realtimenews/<news_id>/<timestamp>.html`

* Once downloaded, we will create a record in Cloud Firestore `/spider/log` with metadata in it.

```yaml
/spider/log/2019/04/17/<timestamp>.html:
  - page_url: "www.chinatimes.com/realtimenews/<news_id>"
  - storage_url: "gs://twnews-rawdata/www.chinatimes.com/realtimenews/<news_id>/<timestamp>.html"
```

## Cloud Function
* From crawled page log generated by Item Pipeline, we use Cloud Function? to extract page detail and store to another Cloud Firestore Collection.
```
/page/:
  - source: "www.chinatimes.com"
  - last_update: "<datetime>"
  - content: "<extracted text>"
  - author: "author name"
  - publish_date: "<datetime>"
  - keywords: ["韓國瑜", "蔡英文", ...]
```

## Middleware
Deduplicate articles that has already crawled before.

Query `/page` to see if a page has already been crawled. 
